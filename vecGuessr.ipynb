{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В этом блокноте происходит предобработка модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запустим Пайморфи для того, чтобы дальше вытащить из модели ТОЛЬКО существительные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "dOuJbDJlld52",
        "outputId": "a6853a97-12a5-449a-9096-5034ebeda615"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Загрузим модель. Использованная модель скачана с [NLPL Word Vectors Repository](http://vectors.nlpl.eu/repository/). Модель (ruwikiruscorpora_upos_skipgram_300_2_2019) обучена на НКРЯ и русской Википедии 2018 года. \n",
        "\n",
        "[Ссылка на скачивание модели](http://vectors.nlpl.eu/repository/20/182.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = KeyedVectors.load_word2vec_format(\"model\\model.txt\", binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "freq_1000.txt -- Файл с 1000-словным списком частотных существительных русского языка (правда, уже не 1000-словный, а подредактированный автором проекта)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq = open(\"freq_1000.txt\", encoding=\"utf8\")\n",
        "freq = freq.read().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Используем библиотеку Navec (из проекта Наташа для NLP), чтобы проверить, что слова, существующие в модели, реально существуют в русском языке (а не являются, например, опечатками). Модель navec обучена на художественных текстах. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from navec import Navec\n",
        "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "navec = Navec.load(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Выберем из модели model.txt только частотные существительные, существующие также и в navec. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "nouns = [] # все существительые модели\n",
        "\n",
        "with open (\"model.txt\", encoding=\"utf8\") as f:\n",
        "    m = f.readlines()\n",
        "\n",
        "    for line in m[1:]:\n",
        "        splitted = line.split(\" \")[0].split(\"_\")\n",
        "        if splitted[1] == \"NOUN\":\n",
        "            s = morph.parse(splitted[0])[0].normal_form\n",
        "            if s in navec:\n",
        "                if s in freq:\n",
        "                    if s not in nouns:\n",
        "                        nouns.append(s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запишем частотные существительные в файл freq_nouns.txt, чтобы потом было из чего выбирать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open (\"freq_nouns.txt\", \"w\", encoding=\"utf-8\") as noun_file:\n",
        "    for n in nouns:\n",
        "        noun_file.write(f\"{n}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Проделаем то же со всеми существительными (уже не отбирая только частотные). Запишем их вектора в файл (не забыв при этом пересчитать мета-данные -- длину модели), а сами существительные -- в список всех существительных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "vec = []\n",
        "noun_all = []\n",
        "\n",
        "with open (\"model.txt\", encoding=\"utf8\") as f:\n",
        "    m = f.readlines()\n",
        "\n",
        "    for line in m[1:]:\n",
        "        splitted = line.split(\" \")[0].split(\"_\")\n",
        "        if splitted[1] == \"NOUN\":\n",
        "            s = morph.parse(splitted[0])[0].normal_form\n",
        "            if s in navec:\n",
        "                if s not in noun_all:\n",
        "                        noun_all.append(s)\n",
        "                        vec.append(line.replace(\"_NOUN\", \"\").replace(splitted[0], s))\n",
        "\n",
        "    with open (\"noun_model.txt\", \"w\", encoding=\"utf8\") as f: # новая модель, содержащая только существительные\n",
        "        f.write(f\"{len(vec)} 300\\n\")\n",
        "        f.write(\"\".join(vec))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запишем все существительные в файл."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open (\"all_nouns.txt\", \"w\", encoding=\"utf-8\") as all_noun_file:\n",
        "    for n in noun_all:\n",
        "        all_noun_file.write(f\"{n}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Итак, у нас получилось 3 файла: \n",
        "\n",
        "`all_nouns.txt` -- все существительные, содержащиеся в модели\n",
        "\n",
        "`freq_nouns.txt` -- частотные существительные, содержащиеся в модели. Из этих существительных будет происходить выбор слов для угадывания (чтобы не пришлось угадывать вещи типа \"диффузия + электрорадиатор\")\n",
        "\n",
        "`noun_model.txt` -- векторная модель, содержащая только существительные"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
